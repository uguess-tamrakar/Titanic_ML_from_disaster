{"cells":[{"metadata":{"_cell_guid":"48a5aeb2-1318-4fb7-85b2-ed73f51410c2","_uuid":"e21ee9437bb657eb1b058674870076c6ca6e6043"},"cell_type":"markdown","source":"I am a beginner to the world of machine and deep learning, and this is my first time attempting a competition at Kaggle. I am going to attempt this based on the knowledge I have gained reading from online sources and few courses. \n\nWithout further ado, letâ€™s dive into it. I am going to divide my work into following sections:\n1. Import and View Data.\n2. Manual Feature Selection.\n3. Analyze and Preprocess Data with Data Visualization\n4. Analysis for algorithm selection. \n5. Creating and Selecting Best Model.\n6. Visualizing the Best Result.\n7. File Submission.\n\nAny comments/suggestions will be greatly appreciated.\n\n**1. Import and View Data**"},{"metadata":{"_cell_guid":"cfdaacbc-23a3-423d-8d4d-120939ac7383","_uuid":"67666103cb6b3f8af34b93f05bba7488e1c016d8","trusted":false,"collapsed":true},"cell_type":"code","source":"# Import pandas that will help us read the provided csv files into dataframes\nimport pandas as pd\ntraining_set = pd.read_csv('../input/train.csv')\ntest_set = pd.read_csv('../input/test.csv')\n\n# Lets view the training set\ntraining_set.head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db2ce274-2dfd-4a05-9b6e-fed0d6d5a737","_uuid":"0670902f3babdcfa0a9645f6a206376942bffcdc","trusted":false,"collapsed":true},"cell_type":"code","source":"# Lets view the test set\ntest_set.head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c19bab1-561b-43f3-987c-a2d9775ee280","_uuid":"244ca15a7e78c3cd722fa5194b8841e0a8f4f7d8"},"cell_type":"markdown","source":"Looking at the data we can see that **Survived** column is the label (dependent variable) and rest of the columns are features (independent variable). Now we will be eliminating those features that are unimportant variable to our model to be trained on."},{"metadata":{"_cell_guid":"e7725dc8-80e2-4023-9777-cbe3857e42f3","_uuid":"3780beca8223f5c92965d45409f125f5be583145"},"cell_type":"markdown","source":"**2. Manual Feature Selection**\n\nLooking at the training and test data above, we can clearly see that we should be able to eliminate few features (columns) because they either do not contribute in training the model or their impact on the prediction will be very insignificant.\n1. PassengerId >>> This feature (also refer to as independent variable) is not relevant and will not give us any information about the survival of the passenger.\n2. Name >>> This feature may contain some relevant data if you consider salutations. For example, take 'Dr.' (Doctor) salutation. One can argue that being of doctor profession they may be more inclined to help and save other passengers' lives risking theirs but provided information doesn't provide enough evidence of that\n3. Ticket >>> This variable in my opinion is redundant because we may be able to extract status information out of it but we can use pclass for this.\n4. Cabin >>> This variable may be useful as depending on the position of the cabin in the ship, one may have more/less probability of survival than others but given the amount of information, we can argue we do not know if passenger were at their cabin or somewhere else during the time of impact.\n5. Fare >>> If you look at the Fare vs Pclass (Ticket class) feature, then you can see that Pclass feature almost represent the Fare feature and it is just redundant data. For example, you will see lower fares for lower class (represented by higher number) and higher fares with higher class."},{"metadata":{"_cell_guid":"c3bda96a-6774-49f7-94d2-b8eb8bab0a93","_uuid":"b4f8a305ddb700df393ff4dd4ed0185c81df3f34","trusted":false,"collapsed":true},"cell_type":"code","source":"# View Fare vs Pclass columns\ntraining_set.iloc[:, [2, 9]].head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ab4c525-a5cb-4183-9468-c1dd005c4c78","collapsed":true,"_kg_hide-output":false,"_uuid":"81ef7cafc06c846cebc67661c7a8bcbfe6343987","_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"# Dropping insignificant/unnecessary columns\ntraining_set = training_set.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis = 1)\ntest_set = test_set.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7faffa7c-9776-43fb-9c01-786630f237ab","_uuid":"69cacc55d0a87b22e50144850b6401a2fb779e14","trusted":false,"collapsed":true},"cell_type":"code","source":"# Lets view the training set now\ntraining_set.head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c159924-6af6-492d-89e6-67e3cfd29202","_uuid":"e0f1b600f638768a199f2907d7c27de1e40d704d","trusted":false,"collapsed":true},"cell_type":"code","source":"# Lets view the test set now\ntest_set.head(n = 10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b1441ec8-7d77-4a69-990b-26e0b1e89b68","_uuid":"906588b2b0134684cb6e6a86353a805ecd353115"},"cell_type":"markdown","source":"**3. Analyze and Preprocess Data with Data Visualization**\n\nI am going to work on this section by dividing it into three sub-sections:\n    1. Get statistical information about the data.\n    2. Analyze the data and Preprocess the data."},{"metadata":{"scrolled":true,"_cell_guid":"e5040262-d76e-44d1-b04b-1bf1756bfd9e","_uuid":"84bef5e290573c55d5a4ad6a4b9e84b853217dad","trusted":false,"collapsed":true},"cell_type":"code","source":"# Get statistical information about training set\ntraining_set.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9633b946-1d16-4e06-bb9c-dd979013e4e1","_uuid":"7f9558bb137c67f784eca6482711304f7ccfafa5","trusted":false,"collapsed":true},"cell_type":"code","source":"# Get statistical information about test set\ntest_set.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"385b8a92-707b-49f3-ba9d-4b6e7e629240","_uuid":"8c92e686b47bb1a1dc504e71f5be5c6a6145caca"},"cell_type":"markdown","source":"Here, the count variable gives us the number of rows with data filled in i.e. non NaN value. We can see that **Age** feature is missing quite a few values (in training and test set) and  **Embarked** feature is missing couple (in training set). So lets fill in these missing values. This process is often called **imputation**.\n\n* **Age**\n    \n    Since Age is a continous numerical feature and it ranges from 0.17 to 76 (extracted from min and max above), we will be filling its missing values with its **mean**."},{"metadata":{"collapsed":true,"_cell_guid":"b1a9e2e1-1718-4e6a-b037-a2c1eca1c003","_uuid":"3882d292b5f011d1b6734a5ba3c9c3320324a2c3","trusted":false},"cell_type":"code","source":"training_set['Age'] = training_set['Age'].fillna(training_set.mean()[0])\ntest_set['Age'] = test_set['Age'].fillna(test_set.mean()[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"669f87db-a8a1-4650-87e6-bcb182acbfe4","_uuid":"1e674c112eb2b92bd503ff95ca7a70bb8218b421"},"cell_type":"markdown","source":"* **Embarked**\n\n    Embarked is a categorical feature with possible values C = Cherbourg, Q = Queenstown, and S = Southampton. We will be filling the missing values with most frequent data i.e. **mode****."},{"metadata":{"collapsed":true,"_cell_guid":"59bd84b7-dce9-4dd4-a69e-226cf3e283e7","_uuid":"9e724a43964835ec6b75a985edbee1196eb2682c","trusted":false},"cell_type":"code","source":"training_set['Embarked'] = training_set['Embarked'].fillna(training_set['Embarked'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d03e2e36-c6a7-47e9-8a60-750b4d7422f9","_uuid":"84b3bbf70b99e67ace9e37c7de225c29fcceb126","trusted":false,"collapsed":true},"cell_type":"code","source":"# Now we can see that Age and Embarked features are not missing any values\nprint(training_set['Age'].count())\nprint(test_set['Age'].count())\nprint(test_set['Embarked'].count())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"22ab0b38-6285-4d65-bb3e-dc923caed94b","_uuid":"224b7c2860c5dddda50fa85a7dae5996a0276e0a"},"cell_type":"markdown","source":"Since now we have a complete data, let's start analyzing data.\nLet's make some manual assumptions about correlation between individual feature and label.\n* **Age Feature:** We can assume that younger passenger may have higher probability to survive than older passenger considering younger passengers are physically more strong. Also, babies may have higher probability of survival. So, we are going to keep this feature to train our model."},{"metadata":{"scrolled":true,"_cell_guid":"952009ab-555c-46f8-b419-182f2de39ca0","_uuid":"9f034334e636173481e1e85f0efe0645e4651c7e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Import data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# draw a bar plot - Age-Group vs. survival\nplt.subplots(1, 1, figsize = (15, 5))\nage_bins = [0, 10, 20, 30 , 40, 50, 60, 70, 80, 90]\nage_group = pd.cut(training_set['Age'], age_bins)\nsurvived = training_set['Survived'].values\nsns.barplot(x = age_group, y = survived)\nplt.xlabel('Age-Group')\nplt.ylabel('Survived')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ef0f0c9d-6b45-4cb0-9026-86b764084398","_uuid":"3cb668a76d2a21f3dcc6fac40849c10ddcb7d9e8"},"cell_type":"markdown","source":"* **Pclass (Ticket class) feature**: We can assume that passenger with high class tickets are more likely to survive and those that are in lower class. Let's see if there is any correlation between Pclass and Survived features."},{"metadata":{"_cell_guid":"2343c32d-d8f3-4216-962d-9371abbd4e89","_uuid":"40aa37b11216b1ecf72194107bbff7ba810d7f87","trusted":false,"collapsed":true},"cell_type":"code","source":"# draw a bar plot - Pclass vs. survival\nplt.subplots(1, 1, figsize = (10, 5))\nsns.barplot(x = 'Pclass', y = 'Survived', data = training_set)\nplt.xlabel('Ticket class')\nplt.ylabel('Survived')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"101f0583-5fa0-4d0b-b81d-7bb60ed1f8b1","_uuid":"8551b75e152dc72f3a55e7aef8e5f082fd380373"},"cell_type":"markdown","source":"* **Sibsp (Sibling and spouse) and Parch (Parents and child) feature: ** We can assume, that having family on board decreases the probability of survival of a passenger. Actually, with the values in these feature we cannot really correlate information based on differences in values of these features vs their survival probability, so we will be combining these features into a feature - **HasFamily**. This process is also refer to as** Feature Engineering**."},{"metadata":{"_cell_guid":"a89c93bb-e45b-44ce-8dee-430f584f4ed4","_uuid":"1e08bdd5b9432c6e45cb3278a1f770618c02b08e","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\n\n# Since number of siblings (sibsp) and parents (parch) denote if passenger had a family onboard\n# we are going to combine these columns to create a column called HasFamily\ntraining_set['HasFamily'] = np.where(training_set['SibSp'] + training_set['Parch'] > 0, 1, 0)\ntest_set['HasFamily'] = np.where(test_set['SibSp'] + test_set['Parch'] > 0, 1, 0)\n# Now we can drop SibSp and Parch columns\ntraining_set = training_set.drop(['SibSp', 'Parch'], axis = 1)\ntest_set = test_set.drop(['SibSp', 'Parch'], axis = 1)\n\ntraining_set.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23c2f140-1dc0-48cd-a6e1-9786510b2606","_uuid":"204458f9421fb8a5b8f5b812d2d00de3bf2704f2","trusted":false,"collapsed":true},"cell_type":"code","source":"test_set.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f126c1f-74b8-4063-8ac0-f44e6b8fc0bd","_uuid":"2b04b5565a11ce5237a996d50575b5d2cd84d74f","trusted":false,"collapsed":true},"cell_type":"code","source":"# draw a factor plot - HasFamily vs. survival\nsns.factorplot(x = 'HasFamily', y = 'Survived', data = training_set)\nplt.xlabel('Has Family')\nplt.ylabel('Survived')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5214295a-19cf-44b5-abe2-8989a0ed9670","_uuid":"c22c000aa8e7d9d0580a398b71c0b71dc9b43997"},"cell_type":"markdown","source":"As we see above that surprisingly, more passengers with family survived the disaster than ones with no family. This will be an important feature to train our model.\n* **Sex**:  Assuming female passengers have more probability of survival than male, lets analyze this feature with data visualization."},{"metadata":{"_cell_guid":"005f14b8-c8c8-4493-8d24-dbb07593b241","_uuid":"35e43a13f67b97022df1ccf655205b1e64e32fc8","trusted":false,"collapsed":true},"cell_type":"code","source":"# draw a bar plot - Sex vs. survival\nsns.barplot(x = 'Sex', y = 'Survived', data = training_set)\nplt.xlabel('Sex')\nplt.ylabel('Survived')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03f6aca5-f105-4855-adf5-ae9a888e160c","_uuid":"d1738f5c30240b67fa9568e371d48eacb880aad6"},"cell_type":"markdown","source":"Now I am going to preprocess some categorical data into numerical form so that we can utilize those feature and run computational algorithms against them."},{"metadata":{"collapsed":true,"_cell_guid":"70da8fc9-aff4-4bb1-9330-806c148a1f2b","_uuid":"4cbf055f0d23b9a691d0967359ac64c67e12f4d1","trusted":false},"cell_type":"code","source":"# Let's divide the data into features and label\nfeatures_train = training_set.iloc[:, 1:].values \nlabels_train = training_set.iloc[:, 0].values\nfeatures_test = test_set.iloc[:, :].values\n# label for test data is not provided\n\n# lets use LabelEncoder to convert Sex data into numerical form\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nfeatures_train[:, 1] = label_encoder.fit_transform(features_train[:, 1])\nfeatures_test[:, 1] = label_encoder.fit_transform(features_test[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b3f6d661-b2a1-48cc-a0d4-b7ddbe17f2cb","_uuid":"04a4e25c9413145f5cd57619d059440c52546fb2","trusted":false},"cell_type":"code","source":"# Now encode Embarked feature\nlabel_encoder = LabelEncoder()\nfeatures_train[:, 3] = label_encoder.fit_transform(features_train[:, 3])\nfeatures_test[:, 3] = label_encoder.fit_transform(features_test[:, 3])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"765dee0b-6f6e-4c55-af34-46b171f195d1","_uuid":"648bc68b21f919f99454d3067d83bb3d5fbcc22d"},"cell_type":"markdown","source":"Now we need to do something called One Hot Encoding to **Embarked** column. One Hot Encoding is done because the label encoding turns the categorical feature into numerical form such as 0, 1, 2, 3, .. so on. Here it assumes that higher the encoded label (as number), better the prediction. In order to eliminate this issue, a feature is turned into number of features depending on the number of unique values in that feature. Here all of the resulting feature values will be in binary form"},{"metadata":{"collapsed":true,"_cell_guid":"f482702d-83e8-472b-96be-1c488f3e1695","_uuid":"1b844a617d1ef8d0bc5c461451b57b32d6dd02ef","trusted":false},"cell_type":"code","source":"# One Hot Encode Embarked feature\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder(categorical_features=[3]) # Embarked Column index is 4\nfeatures_train = one_hot_encoder.fit_transform(features_train).toarray()\none_hot_encoder = OneHotEncoder(categorical_features=[3]) # Embarked Column index is 4\nfeatures_test = one_hot_encoder.fit_transform(features_test).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6e6a70d9-f39e-4d86-b8ed-81070eb8edea","_uuid":"c5c79bcbaf35eb6200947d3ef2bdf8cb8ad19446"},"cell_type":"markdown","source":"Now we need to drop one of the columns created from One Hot Encoding to avoid Dummy Variable Trap. When One Hot Encoding creates these n number of columns they are exposed to a collinearity meaning, one of these encoded feature is can used to easily predict the other as they are in linear form. In order to break this, we drop one of this feature. "},{"metadata":{"collapsed":true,"_cell_guid":"06b242b6-9549-494f-9362-82bffcfaa85f","_uuid":"0e9db6ed7b98ab28df03f05f641861210dd48379","trusted":false},"cell_type":"code","source":"features_train = features_train[:, 1:] # dropping column at 0th index\nfeatures_test = features_test[:, 1:] # dropping column at 0th index","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2b5424c0-196f-4d23-b1b8-1b10ac27be10","_uuid":"1515124f1137b6155fd86c045bfceec56351a686"},"cell_type":"markdown","source":"**4. Analysis for algorithm selection. **\n\nNow that all of the important features have been selected and unnecessary one have been dropped, let's talk about algorithm selection. Since we will be trying various models to get one with best accuracy, we will be dealing with various algorithm but we need to select correct algorithm based on either the problem is Regression problem or Classification problem. We know that this is a Classification problem since the prediction we are making is Survived or not and is not a continous set of predictions like price of a stock. So, lets pick some classification algorithms and train our model."},{"metadata":{"collapsed":true,"_cell_guid":"bf28672b-9264-4d5a-95f8-47effc0e2e4c","_uuid":"56260e8c40b6fe33bc100cbc72c6a2d3e4a70a9f"},"cell_type":"markdown","source":"**5. Creating and Selecting Best Model. **\n\nWe will be fitting and predicting test results by using following classification algorithms:\n1. Logistic Regression\n2. K-Nearest Neighbors\n3. Support Vector Machines\n4. Naive Bayes\n5. Decision Trees\n6. Random Forest\n\nThroughout the process, we will be applying scaling manually to some algorithms and not to others. Scaling is done in order for a feature to not have dominance over other because of the values it contain."},{"metadata":{"_cell_guid":"f19926d7-5059-4394-b4d0-fc8855c60688","_uuid":"237705f6623f1c2d5ae52cd2649fb79b1e0c5d99","trusted":false,"collapsed":true},"cell_type":"code","source":"# Create a dataframe to store algorithms and their accuracies\nalgo_accuracy = pd.DataFrame(columns = ['Algorithm', 'Accuracy'])\n\n# Logistic Regression\n# Scaling features so that one features doesn't dominate the other\nfrom sklearn.preprocessing import StandardScaler\n# Standardize features by removing the mean and scaling to unit variance\n# This basically gets the difference between the value and the mean of values in that column\nscaler = StandardScaler()\nfeatures_train = scaler.fit_transform(features_train)\nfeatures_test = scaler.transform(features_test) #no need to fit since training set already does this\n\n# Fit Logistic Regression to the training data\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = log_reg.predict(features_test)\naccuracy_log_reg = log_reg.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['Logistic Regression', accuracy_log_reg]\nprint(accuracy_log_reg)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"432f4366-8cc4-40a1-a2c6-c8f946e99827","_uuid":"ee17010005a63d5c06145c512d9392772bc94893","trusted":false,"collapsed":true},"cell_type":"code","source":"# K-Nearest Neigbors\n# No need to scale the data since its already done above.\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nKNN.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = KNN.predict(features_test)\naccuracy_KNN = KNN.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['K-Nearest Neighbors', accuracy_KNN]\nprint(accuracy_KNN)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe44808d-4406-4513-afc3-d34050be8b2d","_uuid":"8e98ef90f8acd27b1421c8c0d8216dc5ac8240e0","trusted":false,"collapsed":true},"cell_type":"code","source":"# Support Vector Machines\n# No need to scale the data since its already done above.\nfrom sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf')\nsvc.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = svc.predict(features_test)\naccuracy_svc = svc.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['Support Vector Machines', accuracy_svc]\nprint(accuracy_svc)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55fc9e19-b68f-43df-91d8-579d9adeaabe","_uuid":"a139e193dc7d29738d2b9d6514fc824a1ca5a9ef","trusted":false,"collapsed":true},"cell_type":"code","source":"# Naive Bayes\n# No need to scale the data since its already done above.\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = nb.predict(features_test)\naccuracy_nb = nb.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['Naive Bayes', accuracy_nb]\nprint(accuracy_nb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99721e30-7a87-48d7-86e0-c4682fb47005","_uuid":"d83f8c8b801adb1e62d5c70221eb8eb6499023eb","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Trees\n# No need to scale the data since its already done above.\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy')\ndecision_tree.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = decision_tree.predict(features_test)\naccuracy_decision_tree = decision_tree.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['Decision Trees', accuracy_decision_tree]\nprint(accuracy_decision_tree)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a884aba-adad-46ee-907e-b1fe18cb5f32","_uuid":"c4f45b49cb3d597d3385f76f3c7a3fffe8767f5c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Random Forest\n# No need to scale the data since its already done above.\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators = 20, criterion = 'entropy')\nrandom_forest.fit(features_train, labels_train)\n\n# Predit the test result and calculate accuracy\nlabels_pred = random_forest.predict(features_test)\naccuracy_random_forest = random_forest.score(features_train, labels_train) * 100\nalgo_accuracy.loc[len(algo_accuracy)] = ['Random Forest', accuracy_random_forest]\nprint(accuracy_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f65be836-8ce0-4c74-ab99-5375842717d9","_uuid":"83ea91e70bf1ae50af5675882b6e3d3c12d8db5a"},"cell_type":"markdown","source":"Now, lets put all the algorithms in a table and examine few aspects to derive which is the best algorithm to select a best model for this problem."},{"metadata":{"_cell_guid":"d6682610-26c8-4287-a76d-86806db255cb","_uuid":"474004f66c9085b8a5215631a355cbabb778cfc7","trusted":false,"collapsed":true},"cell_type":"code","source":"algo_accuracy.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"038c4d1b-255f-4813-abd9-0ad515839dfe","_uuid":"5c314b6ea9a30583ddb327d1aa1f3385ec2334d4"},"cell_type":"markdown","source":"Therefore, we can see that Decision Trees was the algorithm with best accuracy. So lets pick the predictions done by Decision Tree and get the file ready for submission."},{"metadata":{"_cell_guid":"d6a0f9dc-a57c-4268-bc7e-16c329baed03","_uuid":"e9993e5ef185ab3b9d468620d2378c7220e50e2a","trusted":false,"collapsed":true},"cell_type":"code","source":"# Decision Trees has the highest accuracy\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy')\ndecision_tree.fit(features_train, labels_train)\n\n# Predit the test result\ndecision_tree_preds = decision_tree.predict(features_test)\n\n# Build a submission file\norig_test_set = pd.read_csv('../input/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": orig_test_set[\"PassengerId\"],\n        \"Survived\": decision_tree_preds\n    })\nsubmission.to_csv('titanic_preds.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_is_fork":false},"nbformat":4,"nbformat_minor":1}